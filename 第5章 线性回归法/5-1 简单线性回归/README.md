写到笔记本吧，便于搜索，这样放到README不知道怎么搜索了
最优化原理和凸优化是什么关系？
哈哈凸优化不就是一种过程抽象吗，哈哈活学活用也。
典型的最小二乘法问题：最小化误差的平方
直接就能搜到结果
有数学思想会使问题更简单？
线性回归算法：
一个特征是标签
解决回归问题
思想简单，实现容易
许多强大的非线性模型的基础
结果具有很好的可解释性
蕴含机器学习中的很多重要思想
假设是线性趋势
拟合样本特征和样本输出标记
分类问题：每个点有两个样本特征
回归：连续的所以占用一个坐标轴
y = ax + b
y^(i) = ax(i) + b
简单线性回归：
假设我们找到了最佳拟合的直线方程：
y = ax + b
则对于每一个样本点x(i)
根据我们的直线方程，预测值为
y^(i) = a(i) + b
真值为：y(i)
使用绝对值，绝对值函数不是处处可导的函数
衡量线性回归法的性能呢，还是用到绝对值
用平方得到了很好的目标

参数学习：几乎所有参数学习算法都是这样的套路
非参数学习算法

线性回归
多项式回归
逻辑回归
SVM
神经网络

模型不同：目标函数不同
凸优化
我们使用计算机解决的很多问题都是最优化问题，比如最小生成树问题
所有最优化就是一种抽象了

看看最优化原理、包括凸优化知识
新的目标函数，此时就要使用凸优化的相关知识了
所以这就是学习凸优化的原因，为了适应更大的范围
典型的最小二乘法问题：最小化误差的平方

